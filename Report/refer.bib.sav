% Encoding: UTF-8

@Online{latex,
  author    = {LaTeX project team},
  date      = {2015-04-03},
  title     = {LaTeX - A document preparation system},
  url       = {https://latex-project.org/},
  urldate   = {2016-02-13},
  publisher = {Frank Mittelbach},
}

@InProceedings{he2016deep,
  author    = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle = {Proceedings of the IEEE conference on computer vision and pattern recognition},
  title     = {Deep residual learning for image recognition},
  year      = {2016},
  pages     = {770--778},
}
@InProceedings{10.1007/978-3-319-46493-0_38,
author="He, Kaiming
and Zhang, Xiangyu
and Ren, Shaoqing
and Sun, Jian",
editor="Leibe, Bastian
and Matas, Jiri
and Sebe, Nicu
and Welling, Max",
title="Identity Mappings in Deep Residual Networks",
booktitle="Computer Vision -- ECCV 2016",
year="2016",
publisher="Springer International Publishing",
address="Cham",
pages="630--645",
abstract="Deep residual networks have emerged as a family of extremely deep architectures showing compelling accuracy and nice convergence behaviors. In this paper, we analyze the propagation formulations behind the residual building blocks, which suggest that the forward and backward signals can be directly propagated from one block to any other block, when using identity mappings as the skip connections and after-addition activation. A series of ablation experiments support the importance of these identity mappings. This motivates us to propose a new residual unit, which makes training easier and improves generalization. We report improved results using a 1001-layer ResNet on CIFAR-10 (4.62 {\%} error) and CIFAR-100, and a 200-layer ResNet on ImageNet. Code is available at: https://github.com/KaimingHe/resnet-1k-layers.",
isbn="978-3-319-46493-0"
}

@Book{garbage,
  title = {《上海市生活垃圾管理条例》},
  year  = {2019},
}

@Misc{krizhevsky2014weird,
  author        = {Alex Krizhevsky},
  title         = {One weird trick for parallelizing convolutional neural networks},
  year          = {2014},
  archiveprefix = {arXiv},
  eprint        = {1404.5997},
  primaryclass  = {cs.NE},
}

@Misc{tan2019efficientnet,
  author        = {Mingxing Tan and Quoc V. Le},
  title         = {EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks},
  year          = {2019},
  archiveprefix = {arXiv},
  eprint        = {1905.11946},
  primaryclass  = {cs.LG},
}
@incollection{NIPS2012_4824,
title = {ImageNet Classification with Deep Convolutional Neural Networks},
author = {Alex Krizhevsky and Sutskever, Ilya and Hinton, Geoffrey E},
booktitle = {Advances in Neural Information Processing Systems 25},
editor = {F. Pereira and C. J. C. Burges and L. Bottou and K. Q. Weinberger},
pages = {1097--1105},
year = {2012},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf}
}

@Article{Sandler_2018,
  author    = {Sandler, Mark and Howard, Andrew and Zhu, Menglong and Zhmoginov, Andrey and Chen, Liang-Chieh},
  journal   = {2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  title     = {MobileNetV2: Inverted Residuals and Linear Bottlenecks},
  year      = {2018},
  month     = {Jun},
  doi       = {10.1109/cvpr.2018.00474},
  isbn      = {9781538664209},
  publisher = {IEEE},
  url       = {http://dx.doi.org/10.1109/CVPR.2018.00474},
}

@Misc{simonyan2014deep,
  author        = {Karen Simonyan and Andrew Zisserman},
  title         = {Very Deep Convolutional Networks for Large-Scale Image Recognition},
  year          = {2014},
  archiveprefix = {arXiv},
  eprint        = {1409.1556},
  primaryclass  = {cs.CV},
}

@Comment{jabref-meta: databaseType:bibtex;}
